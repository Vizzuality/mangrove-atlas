{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0259dfc8",
   "metadata": {},
   "source": [
    "### The way to calculate the coastal length of mangroves for each location is:\n",
    "1.- Buffer extent data by 200m  \n",
    "2.- Clip the buffered extent with the target feature (location)  \n",
    "3.- Merge (union of) the resulting geometries to remove duplicate counting  \n",
    "4.- Clip the coastal extent with the clipped buffered extent polygon  \n",
    "5.- Calculate the length of the clipped coastal extent  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46864000",
   "metadata": {},
   "source": [
    "In order to make things faster we will need to subset the data.  \n",
    "1.- We will create an aggregated buffered by 1km extent layer that will help us reduce the number of multilines the coastal extent layer has to intersect with. This aggregated layer is created in the [next notebook](./../locations-create-mask-mangrove-extent.ipynb)  \n",
    "\n",
    "2.- Another optimization is to spatial intersection each location with the coastal extent dataset so we can attach the location id they belong to. this will help us later to calculate, both the total costal length and the length of the coastal extent for each location.\n",
    "\n",
    "Do we need to simplify the geometries?  \n",
    "How much?  \n",
    "\n",
    "The projection used for calculations is 3410, prjected, and results are in meters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8a8ea",
   "metadata": {},
   "source": [
    "a note on data sources:  \n",
    "* [Mangrove extent data]()\n",
    "* [Mangrove mask](./../locations-create-mask-mangrove-extent.ipynb) This dataset is produced\n",
    "\n",
    "* [Coastal extent data]()\n",
    "* [Locations data](./../locations-api-data.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52775ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "\n",
    "from shapely.geometry import Polygon, box, mapping\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "import shapely.speedups\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from IPython.lib import backgroundjobs as bg\n",
    "\n",
    "shapely.speedups.enable()\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99013004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL PATHS\n",
    "#  FIXME: This will depends from where the notebook kernel is running so be careful\n",
    "WORK_DIR =Path(os.getcwd())\n",
    "BASE_DIR = f'{WORK_DIR.parents[3]}/datasets'\n",
    "\n",
    "# @TODO: Add expected data files source as an environment variable.\n",
    "assert BASE_DIR == '/home/jovyan/work/datasets', f'{BASE_DIR} is not the correct directory'\n",
    "\n",
    "# variables\n",
    "\n",
    "mangrove_extent_path = Path(f'{BASE_DIR}/raw/extent-layer-creation/gmw_v3_fnl_mjr_v314.gpkg')\n",
    "layers = fiona.listlayers(mangrove_extent_path)\n",
    "\n",
    "# Create a connection to the database\n",
    "pg_string = f'postgresql://postgres:postgres@spatial_db:5432/postgres'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "568fa36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(func):\n",
    "    def time_it(*args, **kwargs):\n",
    "        time_started = time.time()\n",
    "        func(*args, **kwargs)\n",
    "        time_elapsed = time.time()\n",
    "        print(f\"{func.__name__} running time is {round(time_elapsed - time_started, 4)} seconds\")\n",
    "    \n",
    "    return time_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3affb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_postgis(layer_name, df, engine):\n",
    "    df.to_postgis(layer_name, engine, if_exists='replace')\n",
    "    return layer_name\n",
    "\n",
    "def file_to_postgis(inputFile: Path, tableName: str, engine):\n",
    "    cmd = f'ogr2ogr -progress \\\n",
    "    -makevalid \\\n",
    "    -overwrite \\\n",
    "    -t_srs EPSG:4326 \\\n",
    "    -lco GEOMETRY_NAME=the_geom \\\n",
    "    --config SPATIAL_INDEX SPGIST  \\\n",
    "    -f \"PostgreSQL\" PG:\"host={engine.url.host} port={engine.url.port} \\\n",
    "    dbname={engine.url.database} user={engine.url.username} password={engine.url.password} active_schema=public\" \\\n",
    "    -nlt PROMOTE_TO_MULTI \\\n",
    "     {inputFile} \\\n",
    "     -nln {tableName};'\n",
    "    execute_command(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f9cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_exists(db_name, engine):\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(\"commit\")\n",
    "        return conn.execute(f\"SELECT 1 FROM pg_database WHERE datname = '{db_name}'\").fetchone() is not None\n",
    "# Create a connection to the database\n",
    "def create_db(db_name, engine):\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(\"commit\")\n",
    "        conn.execute(f\"CREATE DATABASE {db_name};\")\n",
    "\n",
    "def delete_db(db_name, engine):\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(\"commit\")\n",
    "        conn.execute(f\"DROP DATABASE {db_name};\")\n",
    "\n",
    "@measure_time\n",
    "def execute_query(query, connection):\n",
    "    try:\n",
    "        connection.execute(query)\n",
    "        connection.execute(\"commit\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb02513",
   "metadata": {},
   "source": [
    "### Creates a buffered version of the data for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5fb19a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mng_mjr_1996\n",
      "mng_mjr_2007\n",
      "mng_mjr_2008\n",
      "mng_mjr_2009\n",
      "mng_mjr_2010\n",
      "mng_mjr_2015\n",
      "mng_mjr_2016\n",
      "mng_mjr_2017\n",
      "mng_mjr_2018\n",
      "mng_mjr_2019\n",
      "mng_mjr_2020\n"
     ]
    }
   ],
   "source": [
    "for layer_name in layers:\n",
    "    mangrove_extent_df = gpd.read_file(mangrove_extent_path, layer=layer_name\n",
    "                                      ).to_crs('epsg:3410').buffer(200)\n",
    "    gpd.GeoDataFrame({\"geometry\": mangrove_extent_df.clip(gpd.GeoSeries({\"geometry\": box(-180,-50, 180, 40)}, crs='EPSG:4326'\n",
    "        ).to_crs('EPSG:3410')\n",
    "    ).to_crs('EPSG:4326').unary_union}, \n",
    "                     crs='EPSG:4326'\n",
    "    ).to_file(f'{BASE_DIR}/raw/extent-layer-creation/{layer_name}-bufered.shp')\n",
    "    print(f'{layer_name}... created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c00ae",
   "metadata": {},
   "source": [
    "### Prepare a db that uses postgres, uploading the data to it and creating the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7d6d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(pg_string, pool_pre_ping=True, pool_recycle=-1, \n",
    "        connect_args={'connect_timeout': -1, \n",
    "                    \"application_name\":\"coastal_length\"})\n",
    "# Check if the database exists and if not we will create it\n",
    "if not db_exists(engine.url.database, engine):\n",
    "    create_db(engine.url.database, engine)\n",
    "    print(f'Created database {engine.url.database}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea406f",
   "metadata": {},
   "source": [
    "### data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extent layers\n",
    "for layer in layers:\n",
    "    file_to_postgis(f'{BASE_DIR}/raw/extent-layer-creation/{layer}-bufered.shp', f'{layer}-bufered', engine)\n",
    "    print(f'Layer {layer} uploaded to db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a09068f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...10...20...30...40...50...60...70...80...90..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Task created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 - done.\n"
     ]
    }
   ],
   "source": [
    "# Coastallines\n",
    "file_to_postgis(f'{BASE_DIR}/raw/coastlines/coastlines.gpkg', 'coastline', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b629fb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...10...20...30...40...50...60...70.."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Task created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".80...90...100 - done.\n"
     ]
    }
   ],
   "source": [
    "# extent mask data\n",
    "file_to_postgis(f'{BASE_DIR}/raw/extent-layer-creation/merged-convex-bufered-simp-10.shp', 'mask', engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c6862e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...10...20...30...40...50...60...70...80...90..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Task created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 - done.\n"
     ]
    }
   ],
   "source": [
    "# locations data\n",
    "file_to_postgis(f'{BASE_DIR}/processed/locations/locations_v3_gee.shp', 'locations', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e85f56",
   "metadata": {},
   "source": [
    "### Create job manager to upload the data to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7a58c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = bg.BackgroundJobManager() # https://notebook.community/CestDiego/emacs-ipython-notebook/tests/notebook/nbformat4/Background%20Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e486a0",
   "metadata": {},
   "source": [
    "### Creates the coastline subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28faa3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coastline_lengh_location(engine):\n",
    "\n",
    "    query = f\"\"\"\n",
    "    with data as (\n",
    "        select st_Length(st_intersection(c.the_geom, lvg.the_geom)::geography, true) as length, \n",
    "        location_i  \n",
    "        from coastline c\n",
    "        inner join locations lvg on st_intersects(c.the_geom, lvg.the_geom))\n",
    "    select sum(length) coastal_lenght, location_i\n",
    "    into coastline_country\n",
    "    from data group by location_i;\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            execute_query(query, conn)\n",
    "            conn.execute(\"commit\")\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b7f33218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coastline_subset(engine):\n",
    "    \"\"\"Create a subset of the coastline layer to speed up the process\n",
    "    \n",
    "    Args:\n",
    "        engine ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    query_create = \"\"\"\n",
    "    --- Dont forget to ensure parallelization on the heavy queries  \n",
    "    SET max_parallel_workers = 24;\n",
    "    SET max_parallel_workers_per_gather = 24;\n",
    "    SET min_parallel_table_scan_size = '1kB';\n",
    "\n",
    "    -- This query creates a subset of the coastline with the id of each location attached to each line segment  \n",
    "\n",
    "    SELECT st_intersection(c.the_geom, lvg.the_geom) AS the_geom, location_i \n",
    "    INTO coastline_subset \n",
    "    FROM coastline c \n",
    "    INNER JOIN mask tesc ON st_intersects(c.the_geom, tesc.the_geom) \n",
    "    INNER JOIN locations lvg ON st_intersects(c.the_geom, lvg.the_geom);\n",
    "    \"\"\"\n",
    "    query_create_index = \"\"\"\n",
    "    CREATE INDEX coastline_subset_the_geom_gist ON coastline_subset USING SPGIST (the_geom);\n",
    "    CREATE INDEX coastline_subset_location_i_idx ON coastline_subset USING btree (the_geom, location_i);\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            execute_query(query_create, conn)\n",
    "            execute_query(query_create_index, conn)\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69e89262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BackgroundJob #6: <function create_coastline_subset at 0x7f2b4b102a60>>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.dispose()\n",
    "jobs.new(calculate_coastline_lengh_location, engine)\n",
    "jobs.new(create_coastline_subset, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7700ba",
   "metadata": {},
   "source": [
    "### Create the statistics of coastal length for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb0b0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_results(connection):\n",
    "\n",
    "    query_create = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {RESULTS_TABLE_NAME} (\n",
    "        location_id     varchar(255),\n",
    "        year            integer,\n",
    "        value           double precision,\n",
    "        indicator       varchar(255)\n",
    "        )\n",
    "    \"\"\"\n",
    "    return execute_query(query_create, connection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f333554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coastal_length_stats(table, engine):\n",
    "    \"\"\"Create a table with the coastal length stats for each location\n",
    "\n",
    "    Args:\n",
    "        engine ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            year = table.split('_')[-1]\n",
    "            sql = f\"\"\"\n",
    "            SET max_parallel_workers = 24;\n",
    "            SET max_parallel_workers_per_gather = 24;\n",
    "            SET min_parallel_table_scan_size = '1kB';\n",
    "            INSERT INTO {RESULTS_TABLE_NAME} (location_id, year, value, indicator)\n",
    "            SELECT location_i as location_id, \n",
    "                {year} AS year, \n",
    "                SUM(ST_Length(st_intersection(s.the_geom, st_transform(f.the_geom, 4326)), true)) as value, \n",
    "                'linear_coverage' as indicator\n",
    "            FROM coastline_subset s \n",
    "            INNER JOIN \"{table}-bufered\" f ON st_intersects(s.the_geom, st_transform(f.the_geom, 4326))\n",
    "            GROUP BY location_i;\n",
    "            \"\"\"\n",
    "            execute_query(sql, conn)\n",
    "            print(f'process finished: {table}...')\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be99eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create first the table for storing the results if it does not exist\n",
    "RESULTS_TABLE_NAME = 'mangrove_coastal_lenght_stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb29e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as conn:\n",
    "    create_table_results(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f16c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in layers:\n",
    "    jobs.new(create_coastal_length_stats, table, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0a909cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BackgroundJob #11: <function create_coastal_length_stats at 0x7efbfe3055e0>>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "690a2dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing 11 Dead jobs.\n"
     ]
    }
   ],
   "source": [
    "jobs.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4574362",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_coastal_length = gpd.pd.read_sql(f'SELECT * FROM {RESULTS_TABLE_NAME}', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc906f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27896 entries, 0 to 27895\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   location_id  27896 non-null  object \n",
      " 1   year         27896 non-null  int64  \n",
      " 2   value        27896 non-null  float64\n",
      " 3   indicator    27896 non-null  object \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 871.9+ KB\n"
     ]
    }
   ],
   "source": [
    "result_coastal_length.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0794a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_coastal_length.to_csv(f'{BASE_DIR}/processed/coastal_length_stats_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5e6b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.pd.read_sql('select *, (value/coastal_lenght)*100 as perc from coastline_country inner join  mangrove_coastal_lenght_stats mcls on location_id =location_i;', engine).to_csv(f'{BASE_DIR}/processed/coastal_length_stats_v2_full.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
